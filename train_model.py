# -*- coding: utf-8 -*-
"""UAS_NLP_Kelompok7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L1XPNNOZnKyommTCg7d0qNSGL5dbUGjP
"""

import pandas as pd
import re
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import matplotlib.pyplot as plt
import seaborn as sns

!pip install Sastrawi

"""# Data Collection"""

file_path = 'Dataset KDM Barak Militer - Data Full (2).csv'
df = pd.read_csv(file_path)

print("Contoh data:")
print(df.head())
df[20:]

df.info()

df = df.drop(columns=['Unnamed: 2', 'Unnamed: 3'])

df.info()

df.describe()

df.isnull().sum()

df.duplicated().sum()

if 'Sentiment' in df.columns:
    print(df['Sentiment'].value_counts())
    sns.countplot(x='Sentiment', data=df)
    plt.title("Distribusi Label Sentiment")
    plt.show()

df['text_length'] = df['Data'].astype(str).apply(len)
print(df['text_length'].describe())

# Plot distribusi panjang teks
plt.figure(figsize=(8, 4))
sns.histplot(df['text_length'], bins=30, kde=True)
plt.title("Distribusi Panjang Teks")
plt.xlabel("Jumlah Karakter")
plt.show()

from wordcloud import WordCloud

def show_wordcloud(data, title=None):
    text = ' '.join(data.astype(str))
    wordcloud = WordCloud(
        background_color='white',
        max_words=200,
        max_font_size=40,
        scale=3,
        random_state=1
    ).generate(text)

    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    if title: plt.title(title)
    plt.show()

# Wordcloud keseluruhan
show_wordcloud(df['Data'], title="WordCloud - Semua Teks")

# Wordcloud berdasarkan label (jika ada)
if 'Sentiment' in df.columns:
    for label in df['Sentiment'].unique():
        subset = df[df['Sentiment'] == label]
        show_wordcloud(subset['Data'], title=f"WordCloud - {label}")

"""# Data Preprocessing"""

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'http\S+', '', text)  # hapus URL
    text = re.sub(r'@\w+', '', text)     # hapus mention
    text = re.sub(r'#\w+', '', text)     # hapus hashtag
    text = re.sub(r'[^\w\s]', '', text)  # hapus tanda baca
    text = re.sub(r'\d+', '', text)      # hapus angka
    text = re.sub(r'\s+', ' ', text).strip()  # hapus spasi berlebih
    return text

domain_stopwords = {'barak', 'militer', 'kdm', 'anak', 'program', 'mulyadi', 'kang', 'dedi', 'gubernur'}

def remove_domain_stopwords(text):
    return ' '.join([word for word in text.split() if word not in domain_stopwords])

factory = StopWordRemoverFactory()
stopword_list = factory.get_stop_words()

def remove_stopwords(text):
    words = text.split()
    filtered_words = [word for word in words if word not in stopword_list]
    return ' '.join(filtered_words)

def remove_extra_spaces(text):
  return re.sub(r'\s+', ' ', text).strip()

df['clean_tweet'] = df['Data'].apply(clean_text)
df['clean_tweet'] = df['clean_tweet'].apply(remove_domain_stopwords)
df['clean_tweet'] = df['clean_tweet'].apply(remove_stopwords)
df['clean_tweet'] = df['clean_tweet'].apply(remove_extra_spaces)

# contraction_map = {
#     "gak": "tidak",
#     "ga": "tidak",
#     "nggak": "tidak",
#     "gk": "tidak",
#     "kalo": "kalau",
#     "kl": "kalau",
#     "klu": "kalau",
#     "skrg": "sekarang",
#     "skr": "sekarang",
#     "dgn": "dengan",
#     "dg": "dengan",
#     "sm": "sama",
#     "sy": "saya",
#     "gw": "saya",
#     "gua": "saya",
#     "ane": "saya",
#     "aq": "aku",
#     "q": "aku",
#     "loe": "kamu",
#     "lu": "kamu",
#     "elo": "kamu",
#     "lo": "kamu",
#     "km": "kamu",
#     "kmu": "kamu",
#     "tdk": "tidak",
#     "td": "tidak",
#     "bgt": "banget",
#     "bgtu": "begitu",
#     "aj": "saja",
#     "aja": "saja",
#     "sj": "saja",
#     "bbrp": "beberapa",
#     "bbrpa": "beberapa",
#     "dr": "dari",
#     "tp": "tapi",
#     "trus": "terus",
#     "trs": "terus",
#     "lg": "lagi",
#     "udh": "sudah",
#     "sdh": "sudah",
#     "dpt": "dapat",
#     "blm": "belum",
#     "jd": "jadi",
#     "aja": "saja",
#     "kok": "mengapa",
#     "knp": "kenapa",
#     "kenp": "kenapa",
#     "bkn": "bukan",
#     "mnt": "minta",
#     "pls": "tolong",
#     "plis": "tolong",
#     "makasih": "terima kasih",
#     "makasi": "terima kasih",
#     "makasihh": "terima kasih",
#     "thx": "terima kasih",
#     "tq": "terima kasih",
#     "y": "ya",
#     "iya": "ya",
#     "ngga": "tidak",
#     "smua": "semua",
#     "mo": "mau",
#     "ma": "mau",
#     "bgt": "banget",
#     "dl": "dulu",
#     "dlu": "dulu",
#     "bsk": "besok",
#     "lgsg": "langsung",
#     "blg": "bilang",
#     "udh": "sudah",
#     "pd": "pada",
#     "krn": "karena",
#     "krna": "karena",
#     "biar": "supaya",
#     "biarin": "biarkan",
#     "ny": "nya",
#     "bgt": "banget",
#     "jg": "juga",
#     "trnyata": "ternyata",
#     "cmn": "cuma",
#     "cm": "cuma",
#     "doank": "saja",
#     "doin": "melakukan",
#     "dpt": "dapat",
#     "dptin": "dapatkan",
#     "gmn": "gimana",
#     "gmna": "gimana",
#     "gimanaaa": "gimana",
#     "ajaib": "aneh",
#     "aneh2": "aneh-aneh",
#     "trkadang": "terkadang",
#     "hri": "hari",
#     "hr": "hari",
#     "hrn": "heran",
#     "hrs": "harus",
#     "bsa": "bisa",
#     "bs": "bisa",
#     "bisaa": "bisa",
#     "kn": "kan",
#     "knpa": "kenapa",
#     "knpnya": "kenapanya",
#     "ntar": "nanti",
#     "ntaran": "nantian",
#     "ntaps": "mantap",
#     "mntaps": "mantap",
#     "mntp": "mantap",
#     "mntab": "mantap",
#     "mtap": "mantap",
#     "msh": "masih",
#     "msih": "masih",
#     "sklian": "sekalian",
#     "skalian": "sekalian",
#     "sgt": "sangat",
#     "sgtu": "sangat itu",
#     "tuh": "itu",
#     "tu": "itu",
#     "ituu": "itu",
#     "bgtu": "begitu",
#     "gtu": "begitu",
#     "gt": "begitu",
#     "btw": "ngomong-ngomong",
#     "tpn": "tapi kan",
#     "tpi": "tapi",
#     "bknya": "bukannya",
#     "kyk": "seperti",
#     "ky": "seperti",
#     "kaya": "seperti",
#     "sprti": "seperti",
#     "gpp": "tidak apa-apa",
#     "gausah": "tidak usah",
#     "gaperlu": "tidak perlu",
#     "gamasalah": "tidak masalah",
#     "gapapa": "tidak apa-apa",
#     "gabisa": "tidak bisa",
#     "gadak": "tidak ada",
#     "gada": "tidak ada",
#     "gabakal": "tidak akan",
#     "gabakalan": "tidak akan",
#     "gajadi": "tidak jadi",
#     "gakjadi": "tidak jadi",
#     "gajelas": "tidak jelas",
#     "gakjelas": "tidak jelas",
#     "udahh": "sudah",
#     "udahh": "sudah",
#     "udh2": "sudah-sudah",
#     "uda": "sudah",
#     "uda2": "sudah-sudah",
#     "gwpun": "saya pun",
#     "guaa": "saya",
#     "sblm": "sebelum",
#     "slma": "selama",
#     "slm": "selama",
#     "smpe": "sampai",
#     "ampe": "sampai",
#     "ampe2": "sampai-sampai",
#     "drpd": "daripada",
#     "drpdn": "daripadanya",
#     "bgd": "banget",
#     "pgn": "ingin",
#     "pngen": "ingin",
#     "ngen": "ingin",
#     "pngn": "ingin",
#     "brp": "berapa",
#     "brapa": "berapa",
#     "brarti": "berarti",
#     "berati": "berarti",
#     "brti": "berarti",
#     "klh": "kalah",
#     "klrn": "keluarin",
#     "kluar": "keluar",
#     "nnti": "nanti",
#     "lgi": "lagi",
#     "lgian": "lagi-lagi",
#     "trusss": "terus",
#     "teruss": "terus",
#     "trs2": "terus-terusan",
#     "sbnr": "sebenarnya",
#     "sbenernya": "sebenarnya",
#     "sbnarnya": "sebenarnya",
#     "gmnsi": "gimana sih",
#     "bgmna": "bagaimana",
#     "ngpain": "ngapain",
#     "ngapa": "kenapa",
#     "pke": "pakai",
#     "pki": "pakai",
#     "pkai": "pakai",
#     "blanja": "belanja",
#     "ksh": "kasih",
#     "ksih": "kasih",
#     "kasi": "kasih",
#     "nnt": "nanti",
#     "klau": "kalau",
#     "klian": "kalian",
#     "sllu": "selalu",
#     "trkadang": "terkadang",
#     "smpean": "sampai kamu",
#     "dlu2": "dulu-dulu",
# }

# def replace_contractions(text):
#     pattern = re.compile(r'\b(' + '|'.join(contraction_map.keys()) + r')\b')
#     return pattern.sub(lambda x: contraction_map[x.group()], text)

# def remove_punctuation(text):
#     return re.sub(r'[^\w\s]', '', text)

# def case_folding(text):
#     return text.lower()

# def remove_non_alphabet(text):
#     return re.sub(r'[^a-z\s]', '', text)

# def remove_extra_spaces(text):
#     return re.sub(r'\s+', ' ', text).strip()

# stopword_list2 = [
#     "barak", "militer", "kdm", "anak", "program", "mulyadi", "kang", "dedi", "gubernur","ada", "adalah", "adanya", "adapun", "agak", "agaknya", "agar", "akan", "akankah", "akhir", "akhiri", "akhirnya", "aku", "akulah", "amat", "amatlah", "anda", "andalah", "antar", "antara", "antaranya", "apa", "apaan", "apabila", "apakah", "apalagi", "apatah", "artinya", "asal", "asalkan", "atas", "atau", "ataukah", "ataupun", "awal", "awalnya", "bagai", "bagaikan", "bagaimana", "bagaimanakah", "bagaimanapun", "bagi", "bagian", "bahkan", "bahwa", "bahwasanya", "baik", "bakal", "bakalan", "balik", "banyak", "bapak", "baru", "bawah", "beberapa", "begini", "beginian", "beginikah", "beginilah", "begitu", "begitukah", "begitulah", "begitupun", "bekerja", "belakang", "belakangan", "belum", "belumlah", "benar", "benarkah", "benarlah", "berada", "berakhir", "berakhirlah", "berakhirnya", "berapa", "berapakah", "berapalah", "berapapun", "berarti", "berawal", "berbagai", "berdatangan", "beri", "berikan", "berikut", "berikutnya", "berjumlah", "berkali-kali", "berkata", "berkehendak", "berkeinginan", "berkenaan", "berlainan", "berlalu", "berlangsung", "berlebihan", "bermacam", "bermacam-macam", "bermaksud", "bermula", "bersama", "bersama-sama", "bersiap", "bersiap-siap", "bertanya", "bertanya-tanya", "berturut", "berturut-turut", "bertutur", "berujar", "berupa", "besar", "betul", "betulkah", "biasa", "biasanya", "bila", "bilakah", "bisa", "bisakah", "boleh", "bolehkah", "bolehlah", "buat", "bukan", "bukankah", "bukanlah", "bukannya", "bulan", "bung", "cara", "caranya", "cukup", "cukupkah", "cukuplah", "cuma", "dahulu", "dalam", "dan", "dapat", "dari", "daripada", "datang", "dekat", "demi", "demikian", "demikianlah", "dengan", "depan", "di", "dia", "diakhiri", "diakhirinya", "dialah", "diantara", "diantaranya", "diberi", "diberikan", "diberikannya", "dibuat", "dibuatnya", "didapat", "didatangkan", "digunakan", "diibaratkan", "diibaratkannya", "diingat", "diingatkan", "diinginkan", "dijawab", "dijelaskan", "dijelaskannya", "dikarenakan", "dikatakan", "dikatakannya", "dikerjakan", "diketahui", "diketahuinya", "dikira", "dilakukan", "dilalui", "dilihat", "dimaksud", "dimaksudkan", "dimaksudkannya", "dimaksudnya", "diminta", "dimintai", "dimisalkan", "dimulai", "dimulailah", "dimulainya", "dimungkinkan", "dini", "dipastikan", "diperbuat", "diperbuatnya", "dipergunakan", "diperkirakan", "diperlihatkan", "diperlukan", "diperlukannya", "dipersoalkan", "dipertanyakan", "dipunyai", "diri", "dirinya", "disampaikan", "disebut", "disebutkan", "disebutkannya", "disini", "disinilah", "ditambahkan", "ditandaskan", "ditanya", "ditanyai", "ditanyakan", "ditegaskan", "ditujukan", "ditunjuk", "ditunjuki", "ditunjukkan", "ditunjukkannya", "ditunjuknya", "dituturkan", "dituturkannya", "diucapkan", "diucapkannya", "diungkapkan", "dong", "dua", "dulu", "empat", "enggak", "enggaknya", "entah", "entahlah", "guna", "gunakan", "hal", "hampir", "hanya", "hanyalah", "hari", "harus", "haruslah", "harusnya", "hendak", "hendaklah", "hendaknya", "hingga", "ia", "ialah", "ibarat", "ibaratkan", "ibaratnya", "ibu", "ikut", "ingat", "ingat-ingat", "ingin", "inginkah", "inginkan", "ini", "inikah", "inilah", "itu", "itukah", "itulah", "jadi", "jadilah", "jadinya", "jangan", "jangankan", "janganlah", "jauh", "jawab", "jawaban", "jawabnya", "jelas", "jelaskan", "jelaslah", "jelasnya", "jika", "jikalau", "juga", "jumlah", "jumlahnya", "justru", "kala", "kalau", "kalaulah", "kalaupun", "kalian", "kami", "kamilah", "kamu", "kamulah", "kan", "kapan", "kapankah", "kapanpun", "karena", "karenanya", "kasus", "kata", "katakan", "katakanlah", "katanya", "ke", "keadaan", "kebetulan", "kecil", "kedua", "keduanya", "keinginan", "kelamaan", "kelihatan", "kelihatannya", "kelima", "keluar", "kembali", "kemudian", "kemungkinan", "kemungkinannya", "kenapa", "kepada", "kepadanya", "kesampaian", "keseluruhan", "keseluruhannya", "keterlaluan", "ketika", "khususnya", "kini", "kinilah", "kira", "kira-kira", "kiranya", "kita", "kitalah", "kok", "kurang", "lagi", "lagian", "lah", "lain", "lainnya", "lalu", "lama", "lamanya", "lanjut", "lanjutnya", "lebih", "lewat", "lima", "luar", "macam", "maka", "makanya", "makin", "malah", "malahan", "mampu", "mampukah", "mana", "manakala", "manalagi", "masa", "masalah", "masalahnya", "masih", "masihkah", "masing", "masing-masing", "mau", "maupun", "melainkan", "melakukan", "melalui", "melihat", "melihatnya", "memang", "memastikan", "memberi", "memberikan", "membuat", "memerlukan", "memihak", "meminta", "memintakan", "memisalkan", "memperbuat", "mempergunakan", "memperkirakan", "memperlihatkan", "mempersiapkan", "mempersoalkan", "mempertanyakan", "mempunyai", "memulai", "memungkinkan", "menaiki", "menambahkan", "menandaskan", "menanti", "menanti-nanti", "menantikan", "menanya", "menanyai", "menanyakan", "mendapat", "mendapatkan", "mendatang", "mendatangi", "mendatangkan", "menegaskan", "mengakhiri", "mengapa", "mengatakan", "mengatakannya", "mengenai", "mengerjakan", "mengetahui", "menggunakan", "menghendaki", "mengibaratkan", "mengibaratkannya", "mengingat", "mengingatkan", "menginginkan", "mengira", "mengucapkan", "mengucapkannya", "mengungkapkan", "menjadi", "menjawab", "menjelaskan", "menuju", "menunjuk", "menunjuki", "menunjukkan", "menunjuknya", "menurut", "menuturkan", "menyampaikan", "menyangkut", "menyatakan", "menyebutkan", "menyeluruh", "menyiapkan", "merasa", "mereka", "merekalah", "merupakan", "meski", "meskipun", "meyakini", "meyakinkan", "minta", "mirip", "misal", "misalkan", "misalnya", "mula", "mulai", "mulailah", "mulanya", "mungkin", "mungkinkah", "nah", "naik", "namun", "nanti", "nantinya", "nyaris", "nyatanya", "oleh", "olehnya", "pada", "padahal", "padanya", "pak", "paling", "panjang", "pantas", "para", "pasti", "pastilah", "penting", "pentingnya", "per", "percuma", "perlu", "perlukah", "perlunya", "pernah", "persoalan", "pertama", "pertama-tama", "pertanyaan", "pertanyakan", "pihak", "pihaknya", "pukul", "pula", "pun", "punya", "rasa", "rasanya", "rata", "rupanya", "saat", "saatnya", "saja", "sajalah", "saling", "sama", "sama-sama", "sambil", "sampai", "sampai-sampai", "sampaikan", "sana", "sangat", "sangatlah", "satu", "saya", "sayalah", "se", "sebab", "sebabnya", "sebagai", "sebagaimana", "sebagainya", "sebagian", "sebaik", "sebaik-baiknya", "sebaiknya", "sebaliknya", "sebanyak", "sebegini", "sebegitu", "sebelum", "sebelumnya", "sebenarnya", "seberapa", "sebesar", "sebetulnya", "sebisanya", "sebuah", "sebut", "sebutlah", "sebutnya", "secara", "secukupnya", "sedang", "sedangkan", "sedemikian", "sedikit", "sedikitnya", "seenaknya", "segala", "segalanya", "segera", "seharusnya", "sehingga", "seingat", "sejak", "sejauh", "sejenak", "sejumlah", "sekadar", "sekadarnya", "sekali", "sekali-kali", "sekalian", "sekaligus", "sekalipun", "sekarang", "sekarang", "sekecil", "seketika", "sekiranya", "sekitar", "sekitarnya", "sekurang-kurangnya", "sekurangnya", "sela", "selain", "selaku", "selalu", "selama", "selama-lamanya", "selamanya", "selanjutnya", "seluruh", "seluruhnya", "semacam", "semakin", "semampu", "semampunya", "semasa", "semasih", "semata", "semata-mata", "semaunya", "sementara", "semisal", "semisalnya", "sempat", "semua", "semuanya", "semula", "sendiri", "sendirian", "sendirinya", "seolah", "seolah-olah", "seorang", "sepanjang", "sepantasnya", "sepantasnyalah", "seperlunya", "seperti", "sepertinya", "sepihak", "sering", "seringnya", "serta", "serupa", "sesaat", "sesama", "sesampai", "sesegera", "sesekali", "seseorang", "sesuatu", "sesuatunya", "sesudah", "sesudahnya", "setelah", "setempat", "setengah", "seterusnya", "setiap", "setiba", "setibanya", "setidak-tidaknya", "setidaknya", "setinggi", "seusai", "sewaktu", "siap", "siapa", "siapakah", "siapapun", "sini", "sinilah", "soal", "soalnya", "suatu", "sudah", "sudahkah", "sudahlah", "supaya", "tadi", "tadinya", "tahu", "tahun", "tak", "tambah", "tambahnya", "tampak", "tampaknya", "tandas", "tandasnya", "tanpa", "tanya", "tanyakan", "tanyanya", "tapi", "tegas", "tegasnya", "telah", "tempat", "tengah", "tentang", "tentu", "tentulah", "tentunya", "tepat", "terakhir", "terasa", "terbanyak", "terdahulu", "terdapat", "terdiri", "terhadap", "terhadapnya", "teringat", "teringat-ingat", "terjadi", "terjadilah", "terjadinya", "terkira", "terlalu", "terlebih", "terlihat", "termasuk", "ternyata", "tersampaikan", "tersebut", "tersebutlah", "tertentu", "tertuju", "terus", "terutama", "tetap", "tetapi", "tiap", "tiba", "tiba-tiba", "tidak", "tidakkah", "tidaklah", "tiga", "tinggi", "toh", "tunjuk", "turut", "tutur", "tuturnya", "ucap", "ucapnya", "ujar", "ujarnya", "umum", "umumnya", "ungkap", "ungkapnya", "untuk", "usah", "usai", "waduh", "wah", "wahai", "waktu", "waktunya", "walau", "walaupun", "wong", "yaitu", "yakin", "yakni", "yang"
# ]

# # Load stopword dari Sastrawi
# factory = StopWordRemoverFactory()
# stopword_list = factory.get_stop_words()

# def remove_stopwords(text):
#     words = text.split()
#     filtered_words = [word for word in words if word not in stopword_list2]
#     return ' '.join(filtered_words)

# # Lemmatizer / Stemmer Sastrawi
# stemmer = StemmerFactory().create_stemmer()

# def lemmatize(text):
#     return stemmer.stem(text)

# def preprocess_text(text):
#     if not isinstance(text, str):
#         return ""
#     text = clean_text(text)
#     text = replace_contractions(text)
#     text = remove_punctuation(text)
#     text = case_folding(text)
#     text = remove_non_alphabet(text)
#     text = remove_stopwords(text)
#     text = remove_extra_spaces(text)
#     return text

# # Asumsikan kolom tweet kamu bernama 'tweet'
# df['clean_tweet'] = df['Data'].apply(preprocess_text)

# # print("Data setelah preprocessing:")
# print(df[['Data', 'clean_tweet']].head())

df['clean_tweet'][3]

print("Data setelah preprocessing:")
print(df[['Data', 'clean_tweet']].head())

df['clean_tweet']

"""# Data Representation

"""

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding

# --- 1. Data Awal ---
# Gunakan kolom 'clean_tweet' dari DataFrame Anda
kalimat = df['clean_tweet'].tolist()

print("--- Contoh Data Teks Setelah Preprocessing ---")
for i in range(min(5, len(kalimat))): # Tampilkan beberapa contoh
    print(kalimat[i])
print("-" * 25)

# --- 2. Tokenisasi & Padding ---

# a. Tokenisasi
# Membuat tokenizer yang akan memetakan setiap kata ke sebuah angka (integer)
# Sesuaikan num_words jika Anda ingin membatasi ukuran kosakata
tokenizer = Tokenizer(num_words=10000, oov_token="<unk>") # num_words: batas kosakata, oov_token: untuk kata yg tak dikenal
tokenizer.fit_on_texts(kalimat) # "Belajar" setiap kata unik dari data

# Kamus kata yang terbentuk (word -> index)
word_index = tokenizer.word_index
print("Ukuran Kamus Kata (Word Index):", len(word_index))
# print("Kamus Kata (Word Index):") # Bisa dicetak jika ingin melihat isinya
# print(word_index)
print("-" * 25)

# Mengubah setiap kalimat menjadi urutan angka (sekuens)
sequences = tokenizer.texts_to_sequences(kalimat)
print("Jumlah Sekuens (setiap sekuens mewakili satu tweet):", len(sequences))
# print("Contoh Hasil Tokenisasi (Sekuens Angka):") # Bisa dicetak jika ingin melihat isinya
# print(sequences[:5]) # Tampilkan beberapa contoh
print("-" * 25)

# b. Padding
# Menyamakan panjang semua sekuens.
# Tentukan MAX_SEQUENCE_LENGTH sesuai kebutuhan Anda
MAX_SEQUENCE_LENGTH = 60 # Sesuaikan dengan panjang tweet Anda setelah preprocessing

padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')
print(f"Hasil Padding (Panjang Seragam: {MAX_SEQUENCE_LENGTH}):")
print("Bentuk array padding:", padded_sequences.shape)
# print("Contoh Hasil Padding:") # Bisa dicetak jika ingin melihat isinya
# print(padded_sequences[:5])
print("-" * 25)

# --- 3. Word Embedding ---
# Mengubah sekuens angka menjadi vektor padat (dense vectors)

# Ukuran kosakata (ditambah 1 untuk padding)
vocab_size = len(word_index) + 1
# Dimensi vektor untuk setiap kata (misal, setiap kata diwakili oleh 8 angka)
embedding_dim = 100 # Anda bisa mencoba dimensi yang berbeda
# Panjang maksimal sekuens dari hasil padding (sudah ditentukan di sel sebelumnya)
# max_length = padded_sequences.shape[1] # Ambil dari shape hasil padding

# Membuat model sederhana hanya dengan lapisan Embedding
# Ini adalah contoh dasar. Untuk tugas sentimen, Anda akan membangun model yang lebih kompleks
# di bagian "Modelling" menggunakan hasil embedding ini atau langsung menggunakan model BERT.
embedding_layer = Embedding(input_dim=vocab_size,         # Ukuran kosakata
                            output_dim=embedding_dim,     # Dimensi output vektor
                            input_length=MAX_SEQUENCE_LENGTH) # Panjang input sekuens

# Contoh penggunaan layer embedding (opsional, hanya untuk melihat output)
# Ini akan menghasilkan array 3D (jumlah_sampel, panjang_sekuens, embedding_dim)
# embedding_output = embedding_layer(padded_sequences)

print("--- Informasi Layer Embedding ---")
print(f"Ukuran Kosakata (input_dim): {vocab_size}")
print(f"Dimensi Embedding (output_dim): {embedding_dim}")
print(f"Panjang Sekuens (input_length): {MAX_SEQUENCE_LENGTH}")
# print("\nContoh Bentuk Output Embedding (untuk batch pertama):", embedding_output[:1].shape)
print("Layer Embedding siap digunakan dalam model Anda.")
print("-" * 25)

"""# Modelling"""

from huggingface_hub import login
login()

pip install tensorflow transformers scikit-learn numpy

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import seaborn as sns

# Pastikan kolom sesuai
tweets = df['clean_tweet'].astype(str).tolist()
sentiment_mapping = {'Negatif': 0, 'Netral': 1, 'Positif': 2}
labels = df['Sentiment'].map(sentiment_mapping).tolist()

# --- Konfigurasi ---
MODEL_NAME = "Aardiiiiy/indobertweet-base-Indonesian-sentiment-analysis"
MAX_SEQUENCE_LENGTH = 60
NUM_CLASSES = 3
LEARNING_RATE = 2e-5
EPOCHS = 20
BATCH_SIZE = 32

# --- Split Data ---
X_train, X_test, y_train, y_test = train_test_split(
    tweets, labels, test_size=0.2, stratify=labels, random_state=42
)

# --- Tokenizer ---
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def encode_texts(tokenizer, texts, max_len):
    return tokenizer(
        texts,
        max_length=max_len,
        padding='max_length',
        truncation=True,
        return_tensors='tf'
    )

train_encodings = encode_texts(tokenizer, X_train, MAX_SEQUENCE_LENGTH)
test_encodings = encode_texts(tokenizer, X_test, MAX_SEQUENCE_LENGTH)

# --- One-hot Encoding Labels ---
y_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=NUM_CLASSES)
y_test_oh = tf.keras.utils.to_categorical(y_test, num_classes=NUM_CLASSES)

# --- Load Model ---
model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_CLASSES)
from transformers import create_optimizer

# Optimizer dan scheduler dari Hugging Face
steps_per_epoch = len(X_train) // BATCH_SIZE
num_train_steps = steps_per_epoch * EPOCHS
optimizer, schedule = create_optimizer(
    init_lr=LEARNING_RATE,
    num_warmup_steps=0,
    num_train_steps=num_train_steps
)

model.compile(
    optimizer=optimizer,
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Hitung class weights
# y_train adalah list label Anda sebelum di one-hot encode (misal: [0, 1, 2, 1, 0, ...])
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
# Ubah menjadi dictionary agar bisa digunakan Keras
class_weights_dict = dict(enumerate(class_weights))

print(f"Class Weights: {class_weights_dict}")
# Outputnya akan seperti: {0: 1.2, 1: 0.8, 2: 1.25} (angka hanya contoh)

# --- Training ---
history = model.fit(
    x={'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},
    y=y_train_oh,
    validation_split=0.1,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    class_weight=class_weights_dict
)

# --- Evaluation ---
loss, acc = model.evaluate(
    x={'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']},
    y=y_test_oh,
    batch_size=BATCH_SIZE
)
print(f"\n📊 Test Loss: {loss:.4f} | Test Accuracy: {acc:.4f}")

# --- Prediction ---
raw_preds = model.predict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']})
pred_probs = tf.nn.softmax(raw_preds.logits, axis=1).numpy()
y_pred = np.argmax(pred_probs, axis=1)

# --- Classification Report ---
print("\n📋 Classification Report:")
print(classification_report(y_test, y_pred, target_names=sentiment_mapping.keys(), digits=4))

# --- Confusion Matrix ---
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=sentiment_mapping.keys(),
            yticklabels=sentiment_mapping.keys())
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

# --- Training Curve ---
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# --- Prediksi pada Teks Baru ---
def predict_texts(texts):
    enc = encode_texts(tokenizer, texts, MAX_SEQUENCE_LENGTH)
    raw = model.predict({'input_ids': enc['input_ids'], 'attention_mask': enc['attention_mask']})
    probs = tf.nn.softmax(raw.logits, axis=1).numpy()
    preds = np.argmax(probs, axis=1)
    inv_map = {v: k for k, v in sentiment_mapping.items()}

    for i, text in enumerate(texts):
        print(f"\n📨 Tweet: {text}")
        print(f"📈 Probabilitas: {probs[i]}")
        print(f"🧭 Prediksi Sentimen: {inv_map[preds[i]]}")

# --- Contoh Prediksi ---
new_tweets = [
    "Barak ini sangat kotor dan bau, sungguh memprihatinkan.",
    "Latihan militer hari ini berjalan lancar.",
    "Saya dengar ada kunjungan jenderal ke barak besok."
]
predict_texts(new_tweets)

# --- Save Trained Model ---
import os
os.makedirs("model", exist_ok=True)
model.save_pretrained("model/indobert_bilstm")
tokenizer.save_pretrained("model/indobert_bilstm")
print("✅ Model dan tokenizer berhasil disimpan ke folder model/indobert_bilstm")

# Save the model in TensorFlow SavedModel format
model.save("bert_sentiment_model", save_format="tf")

# If you only want to save the weights, you can use:
# model.save_weights("bert_sentiment_weights.h5")

# import tensorflow as tf
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report, confusion_matrix
# from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
# import matplotlib.pyplot as plt
# import seaborn as sns

# # --- Load Dataset ---
# tweets = df['clean_tweet'].astype(str).tolist()

# # Mapping label sentimen ke angka
# sentiment_mapping = {'Negatif': 0, 'Netral': 1, 'Positif': 2}
# labels = df['Sentiment'].map(sentiment_mapping).tolist()

# # --- Konfigurasi ---
# MODEL_NAME = "Aardiiiiy/indobertweet-base-Indonesian-sentiment-analysis"
# MAX_SEQUENCE_LENGTH = 60
# NUM_CLASSES = 3
# LEARNING_RATE = 2e-5
# EPOCHS = 3
# BATCH_SIZE = 32

# # --- Split Data ---
# X_train, X_test, y_train, y_test = train_test_split(
#     tweets, labels, test_size=0.2, stratify=labels, random_state=42
# )

# # --- Tokenizer ---
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# def encode_texts(tokenizer, texts, max_len):
#     return tokenizer(
#         texts,
#         max_length=max_len,
#         padding='max_length',
#         truncation=True,
#         return_tensors='tf'
#     )

# train_encodings = encode_texts(tokenizer, X_train, MAX_SEQUENCE_LENGTH)
# test_encodings = encode_texts(tokenizer, X_test, MAX_SEQUENCE_LENGTH)

# # --- One-hot Encoding Label ---
# y_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=NUM_CLASSES)
# y_test_oh = tf.keras.utils.to_categorical(y_test, num_classes=NUM_CLASSES)

# # --- Load Model ---
# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_CLASSES)
# model.compile(
#     optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
#     loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
#     metrics=['accuracy']
# )

# # --- Training ---
# history = model.fit(
#     x={'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},
#     y=y_train_oh,
#     validation_split=0.1,
#     batch_size=BATCH_SIZE,
#     epochs=EPOCHS
# )

# # --- Evaluation ---
# loss, acc = model.evaluate(
#     x={'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']},
#     y=y_test_oh,
#     batch_size=BATCH_SIZE
# )
# print(f"\nTest Loss: {loss:.4f}, Test Accuracy: {acc:.4f}")

# # --- Prediction ---
# raw_preds = model.predict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']})
# pred_probs = tf.nn.softmax(raw_preds.logits, axis=1).numpy()
# y_pred = np.argmax(pred_probs, axis=1)

# # --- Classification Report & Confusion Matrix ---
# print("\nClassification Report:")
# print(classification_report(y_test, y_pred, digits=4))

# cm = confusion_matrix(y_test, y_pred)
# plt.figure(figsize=(6, 5))
# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sentiment_mapping.keys(), yticklabels=sentiment_mapping.keys())
# plt.xlabel("Predicted")
# plt.ylabel("True")
# plt.title("Confusion Matrix")
# plt.show()

# # --- Training Curve ---
# plt.figure(figsize=(12, 5))
# plt.subplot(1, 2, 1)
# plt.plot(history.history['accuracy'], label='Train')
# plt.plot(history.history['val_accuracy'], label='Val')
# plt.title('Accuracy')
# plt.legend()

# plt.subplot(1, 2, 2)
# plt.plot(history.history['loss'], label='Train')
# plt.plot(history.history['val_loss'], label='Val')
# plt.title('Loss')
# plt.legend()
# plt.tight_layout()
# plt.show()

# # --- Prediksi pada Teks Baru ---
# def predict_texts(texts):
#     enc = encode_texts(tokenizer, texts, MAX_SEQUENCE_LENGTH)
#     raw = model.predict({'input_ids': enc['input_ids'], 'attention_mask': enc['attention_mask']})
#     probs = tf.nn.softmax(raw.logits, axis=1).numpy()
#     preds = np.argmax(probs, axis=1)
#     inv_map = {v: k for k, v in sentiment_mapping.items()}
#     for i, text in enumerate(texts):
#         print(f"\nTeks: {text}")
#         print(f"Probabilitas: {probs[i]}")
#         print(f"Prediksi: {inv_map[preds[i]]}")

# # Contoh
# new_tweets = [
#     "Barak ini sangat kotor dan bau, sungguh memprihatinkan.",
#     "Latihan militer hari ini berjalan lancar.",
#     "Saya dengar ada kunjungan jenderal ke barak besok."
# ]
# predict_texts(new_tweets)

# import tensorflow as tf
# import numpy as np
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report, confusion_matrix
# from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
# import matplotlib.pyplot as plt
# import seaborn as sns

# # --- Konfigurasi Model dan Data ---
# # Tentukan nama model IndoBERTweet yang akan digunakan
# # "indobenchmark/indobert-tweet-base-uncased" adalah pilihan yang bagus untuk tweet.
# MODEL_NAME = "indobenchmark/indobert-base-uncased-finetuned-sentiment"

# # Panjang maksimum sequence. Sesuaikan dengan panjang tweet Anda setelah preprocessing.
# # Kebanyakan tweet pendek, 60-128 mungkin cukup.
# MAX_SEQUENCE_LENGTH = 60

# # Jumlah kelas sentimen Anda (misal: 2 untuk Positif/Negatif, 3 untuk Positif/Netral/Negatif)
# NUM_CLASSES = 3 # Contoh: 0=Negatif, 1=Netral, 2=Positif

# # Hyperparameter pelatihan
# LEARNING_RATE = 2e-5
# EPOCHS = 3 # Untuk fine-tuning BERT, biasanya 2-5 epoch sudah cukup
# BATCH_SIZE = 32 # Sesuaikan dengan kapasitas RAM/GPU Anda

# # --- Data Dummy (Ganti dengan Data Asli Anda) ---
# # Contoh data tweet dan label sentimen
# # Pastikan teksnya sudah bersih dari URL, username, hashtag (jika tidak relevan).
# # Label harus dalam bentuk integer (0, 1, 2, dst).
# # Using the clean_tweet column from the dataframe and creating labels based on the Sentiment column
# tweets = df['clean_tweet'].tolist()

# # Mapping sentiment labels to integers
# sentiment_mapping = {'Negatif': 0, 'Netral': 1, 'Positif': 2}
# labels = df['Sentiment'].map(sentiment_mapping).tolist()


# # --- Membagi Data menjadi Training dan Testing ---
# # Kita bagi data menjadi 80% training dan 20% testing
# X_train, X_test, y_train, y_test = train_test_split(
#     tweets, labels, test_size=0.2, random_state=42, stratify=labels
# )

# print(f"Jumlah data training: {len(X_train)}")
# print(f"Jumlah data testing: {len(X_test)}")

# # --- Langkah 1: Inisialisasi Tokenizer ---
# # Tokenizer BERT akan mengubah teks menjadi format yang bisa dimengerti model BERT.
# # Ini termasuk penambahan token khusus (CLS, SEP), konversi ke ID, dan attention mask.
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# print("\n--- Tokenizer berhasil diinisialisasi ---")

# # --- Langkah 2: Tokenisasi dan Encoding Data ---
# # Fungsi untuk meng-encode batch teks
# def encode_texts(tokenizer, texts, max_len):
#     encoded_batch = tokenizer.batch_encode_plus(
#         texts,
#         max_length=max_len,
#         padding='max_length',
#         truncation=True,
#         return_tensors='tf' # Mengembalikan tensor TensorFlow
#     )
#     return {
#         'input_ids': encoded_batch['input_ids'],
#         'attention_mask': encoded_batch['attention_mask']
#     }

# print("\n--- Melakukan encoding pada data training dan testing ---")
# train_encodings = encode_texts(tokenizer, X_train, MAX_SEQUENCE_LENGTH)
# test_encodings = encode_texts(tokenizer, X_test, MAX_SEQUENCE_LENGTH)

# # Mengonversi label ke format one-hot encoding jika classification_mode='softmax'
# # TFAutoModelForSequenceClassification sudah mengharapkan label dalam format ini
# # jika from_logits=False pada loss function.
# # Namun, jika from_logits=True (default untuk TFAutoModelForSequenceClassification),
# # label bisa tetap integer. Untuk simplicity, kita pakai one-hot karena lebih universal.
# y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=NUM_CLASSES)
# y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes=NUM_CLASSES)

# print(f"Shape input_ids training: {train_encodings['input_ids'].shape}")
# print(f"Shape attention_mask training: {train_encodings['attention_mask'].shape}")
# print(f"Shape label training: {y_train_one_hot.shape}")

# # --- Langkah 3: Membangun dan Memuat Model IndoBERTweet ---
# # TFAutoModelForSequenceClassification akan secara otomatis menambahkan
# # lapisan klasifikasi di atas model pre-trained.
# # num_labels harus sesuai dengan jumlah kelas sentimen Anda.
# print(f"\n--- Memuat model IndoBERTweet: {MODEL_NAME} dengan {NUM_CLASSES} kelas ---")
# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_CLASSES)

# # Menampilkan ringkasan model
# model.summary()

# # --- Langkah 4: Compile Model ---
# # Kita gunakan optimizer Adam dengan learning rate yang kecil (khas untuk fine-tuning BERT)
# # Loss function: CategoricalCrossentropy jika lebih dari 2 kelas, BinaryCrossentropy jika 2 kelas.
# # from_logits=True karena output dari TFAutoModelForSequenceClassification biasanya belum melalui aktivasi softmax/sigmoid.
# optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)

# if NUM_CLASSES > 2:
#     loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
# else: # Binary classification
#     loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)

# metrics = ['accuracy']

# model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)
# print("\n--- Model berhasil dicompile ---")

# # --- Langkah 5: Melatih Model (Fine-tuning) ---
# print(f"\n--- Memulai pelatihan (fine-tuning) selama {EPOCHS} epoch ---")
# history = model.fit(
#     x={'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},
#     y=y_train_one_hot,
#     epochs=EPOCHS,
#     batch_size=BATCH_SIZE,
#     validation_split=0.1, # Menggunakan 10% dari data training untuk validasi
#     verbose=1
# )
# print("\n--- Pelatihan selesai ---")

# # --- Langkah 6: Evaluasi Model ---
# print("\n--- Mengevaluasi model pada data testing ---")
# loss, accuracy = model.evaluate(
#     x={'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']},
#     y=y_test_one_hot,
#     batch_size=BATCH_SIZE,
#     verbose=0
# )
# print(f"Loss pada data test: {loss:.4f}")
# print(f"Akurasi pada data test: {accuracy:.4f}")

# # --- Mendapatkan Prediksi dan Laporan Klasifikasi ---
# print("\n--- Membuat prediksi pada data testing ---")
# raw_predictions = model.predict(
#     {'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}
# )

# # Konversi logit (output raw) menjadi probabilitas (jika NUM_CLASSES > 1)
# if NUM_CLASSES > 1:
#     probabilities = tf.nn.softmax(raw_predictions.logits, axis=-1).numpy()
#     predicted_labels = np.argmax(probabilities, axis=1)
# else: # Binary classification
#     probabilities = tf.nn.sigmoid(raw_predictions.logits).numpy()
#     predicted_labels = (probabilities > 0.5).astype(int).flatten()

# # Tampilkan laporan klasifikasi
# print("\n--- Laporan Klasifikasi ---")
# print(classification_report(y_test, predicted_labels, digits=4))

# # Tampilkan Confusion Matrix
# print("\n--- Confusion Matrix ---")
# cm = confusion_matrix(y_test, predicted_labels)
# print(cm)

# plt.figure(figsize=(8, 6))
# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
#             xticklabels=[f"Class {i}" for i in range(NUM_CLASSES)],
#             yticklabels=[f"Class {i}" for i in range(NUM_CLASSES)])
# plt.xlabel('Predicted Label')
# plt.ylabel('True Label')
# plt.title('Confusion Matrix')
# plt.show()

# # --- Visualisasi Hasil Pelatihan ---
# print("\n--- Memvisualisasikan hasil pelatihan ---")
# plt.figure(figsize=(12, 5))

# plt.subplot(1, 2, 1)
# plt.plot(history.history['accuracy'], label='Training Accuracy')
# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
# plt.title('Training and Validation Accuracy')
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.legend()

# plt.subplot(1, 2, 2)
# plt.plot(history.history['loss'], label='Training Loss')
# plt.plot(history.history['val_loss'], label='Validation Loss')
# plt.title('Training and Validation Loss')
# plt.xlabel('Epoch')
# plt.ylabel('Loss')
# plt.legend()

# plt.tight_layout()
# plt.show()

# # --- Contoh Prediksi pada Teks Baru ---
# print("\n--- Contoh Prediksi Teks Baru ---")
# new_tweets = [
#     "Barak ini sangat kotor dan bau, sungguh memprihatinkan.",
#     "Latihan militer hari ini berjalan lancar.",
#     "Saya dengar ada kunjungan jenderal ke barak besok."
# ]

# new_encodings = encode_texts(tokenizer, new_tweets, MAX_SEQUENCE_LENGTH)
# new_raw_predictions = model.predict(
#     {'input_ids': new_encodings['input_ids'], 'attention_mask': new_encodings['attention_mask']}
# )

# if NUM_CLASSES > 1:
#     new_probabilities = tf.nn.softmax(new_raw_predictions.logits, axis=-1).numpy()
#     new_predicted_labels = np.argmax(new_probabilities, axis=1)
# else:
#     new_probabilities = tf.nn.sigmoid(new_raw_predictions.logits).numpy()
#     new_predicted_labels = (new_probabilities > 0.5).astype(int).flatten()

# sentiment_map = {0: "Negatif", 1: "Netral", 2: "Positif"} # Sesuaikan dengan mapping label Anda

# for i, tweet in enumerate(new_tweets):
#     print(f"Tweet: '{tweet}'")
#     print(f"Probabilitas: {new_probabilities[i]}")
#     print(f"Sentimen Prediksi: {sentiment_map[new_predicted_labels[i]]}\n")

"""# Streamlit"""

!pip install -q streamlit pyngrok transformers scikit-learn Sastrawi pandas

!pip install -q transformers

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import re
# from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
# from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
# import tensorflow as tf
# import numpy as np
# 
# # --- Preprocessing ---
# def clean_text(text):
#     text = str(text).lower()
#     text = re.sub(r"http\S+", "", text)
#     text = re.sub(r"@\w+", "", text)
#     text = re.sub(r"#\w+", "", text)
#     text = re.sub(r"[^\w\s]", "", text)
#     text = re.sub(r"\d+", "", text)
#     text = re.sub(r"\s+", " ", text).strip()
#     return text
# 
# domain_stopwords = {'barak', 'militer', 'kdm', 'anak', 'program', 'mulyadi', 'kang', 'dedi', 'gubernur'}
# 
# def remove_domain_stopwords(text):
#     return ' '.join([word for word in text.split() if word not in domain_stopwords])
# 
# factory = StopWordRemoverFactory()
# stopword_list = factory.get_stop_words()
# 
# def remove_stopwords(text):
#     return ' '.join([word for word in text.split() if word not in stopword_list])
# 
# def preprocess_text(text):
#     text = clean_text(text)
#     text = remove_domain_stopwords(text)
#     text = remove_stopwords(text)
#     return text
# 
# # --- Load Model & Tokenizer ---
# MODEL_NAME = "Aardiiiiy/indobertweet-base-Indonesian-sentiment-analysis"
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
# 
# sentiment_mapping = {0: "Negatif", 1: "Netral", 2: "Positif"}
# 
# # --- Streamlit UI ---
# st.title("🇮🇩 Prediksi Sentimen IndoBERTweet")
# text = st.text_area("Masukkan tweet Anda:")
# 
# if st.button("Prediksi Sentimen"):
#     if text.strip() == "":
#         st.warning("Teks kosong!")
#     else:
#         clean = preprocess_text(text)
#         enc = tokenizer([clean], padding="max_length", truncation=True, max_length=60, return_tensors="tf")
#         logits = model(enc)[0]
#         probs = tf.nn.softmax(logits, axis=1).numpy()
#         pred = np.argmax(probs)
# 
#         st.markdown(f"### 🧭 Prediksi: **{sentiment_mapping[pred]}**")
#         st.write("📊 Probabilitas:")
#         st.json({sentiment_mapping[i]: float(p) for i, p in enumerate(probs[0])})
#

from pyngrok import conf, ngrok

# Masukkan authtoken kamu di sini (ganti string-nya)
conf.get_default().auth_token = "308rlhQXlzWTYyecEstXd5CnkoD_5sEPULzRSg1LZJN7FsKRv"

from pyngrok import ngrok

# Jalankan Streamlit app di background
!streamlit run app.py &>/content/log.txt &

# Akses melalui ngrok
url = ngrok.connect(8501)
print("🌐 Streamlit App URL:", url)